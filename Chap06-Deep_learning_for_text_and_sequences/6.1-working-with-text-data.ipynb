{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap06 - 텍스트와 시퀀스를 위한 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- 텍스트 데이터를 유용한 형태로 전처리하는 방법\n",
    "- 순환 신경망(RNN)을 사용하는 방법\n",
    "- 1D 컨브넷을 사용한 시퀀스 데이터의 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "- 시퀀스(sequence) 데이터를 처리하는 기본적인 딥러닝 모델을 **순환 신경망(RNN, Recurrent Neural Network)**과 **1D 컨브넷(1D convnet)** 두 가지다.\n",
    "  - 1D 컨브넷은 2D 컨브넷의 1차원 버전이다.\n",
    "- 다음과 같은 예제에 사용할 수 있다.\n",
    "  - 문서 분류나 시계열 분류. → 글의 주제나 책의 저자 식별하기\n",
    "  - 시계열 비교. → 두 문서나 두 주식 가격이 얼마나 밀접하게 관련이 있는지 추정하기\n",
    "  - 시퀀스-투-시퀀스 학습. → 영어 문장을 한국어로 번역\n",
    "  - 감성 분석 → 영화 리뷰 긍정/부정 분류하기\n",
    "  - 시계열 예측 → 향후 날씨 예측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 텍스트 데이터 다루기\n",
    "\n",
    "- 텍스트는 가장 흔한 시퀀스 형태의 데이터다.\n",
    "  - 텍스트는 단어의 시퀀스나 문자의 시퀀스로 이해할 수 있다.\n",
    "  - 보통 단어 수준으로 작업하는 경우가 많다.\n",
    "\n",
    "- 시퀀스 처리용 딥러닝 모델은 텍스트를 사용하여 기초적인 자연어 이해(NLU, natural language understanding) 문제를 처리할 수 있다.\n",
    "- 이러한 모델은 문자 언어(written language)에 대한 **통계적 구조**를 만들어 간단한 텍스트 문제를 해결한다.\n",
    "- 자연어 처리(NLP, natural language processing)를 위한 딥러닝은 **단어, 문장, 문단**에 적용한 패턴인식이다.\n",
    "- 딥러닝 모델의 입력으로 텍스트 원본을 사용하지 못하기 때문에 **텍스트 → 텐서** 로 변환해줘야 한다 → **텍스트 벡터화(vectorizing text)**\n",
    "- 텍스트 벡터화에는 여러가지 방식이 있다.\n",
    "  - 텍스트를 **단어(word)**로 나누고 각 단어를 하나의 벡터로 변환한다.\n",
    "  - 텍스트를 **문자(character)** 로 나누고 각 문자를 하나의 벡터로 변환한다.\n",
    "  - 텍스트에서 단어나 문자의 **n-gram**을 추출하여 각 n-gram을 하나의 벡터로 변환한다.\n",
    "- 텍스트를 나누는 단위(단어, 문자, n-gram)을 **토큰(token)**이라고 하며, 토큰으로 나누는 작업을 **토큰화(tokenization)** 라고 한다.\n",
    "- 모든 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 **생성된 토큰에 벡터를 연결하는 것** 으로 이루어진다.\n",
    "  - 이 벡터는 시퀀스 텐서로 묶여져서 신경망에 주입된다.\n",
    "- 토큰과 벡터를 연결하는 방법은 여러가지가 있는데, 대표적인 두 가지는 다음과 같다.\n",
    "  - **원-핫 인코딩(one-hot encoding)**\n",
    "  - **토큰 임베딩(token embedding)** = **단어 임베딩(word embedding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 단어와 문자의 원-핫 인코딩\n",
    "\n",
    "원-핫 인코딩은 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법입니다. [3장에서 IMDB](https://github.com/ExcelsiorCJH/Deep-Learning-with-Python/blob/master/Chap03-Getting_started_with_neural_networks/3.4-classifying_movie_reviews.ipynb)와 [로이터 예제](https://github.com/ExcelsiorCJH/Deep-Learning-with-Python/blob/master/Chap03-Getting_started_with_neural_networks/3.5-classifying_newswires.ipynb)에서 이를 보았습니다(단어의 원-핫 인코딩을 사용했습니다). \n",
    "\n",
    "모든 단어에 **고유한 정수 인덱스**를 부여하고 이 정수 인덱스 i를 크기가 N(어휘 사전의 크기)인 이진 벡터로 변환합니다. 이 벡터는 **i번째 원소만 1이고 나머지는 모두 0**입니다.\n",
    "\n",
    "물론 원-핫 인코딩은 문자 수준에서도 적용할 수 있습니다. 원-핫 인코딩이 무엇이고 어떻게 구현하는지 명확하게 설명하기 위해 단어와 문자에 대한 간단한 예를 만들었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 수준의 원-핫 인코딩(간단한 예):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 초기 데이터: 각 원소가 샘플입니다\n",
    "# (이 예에서 하나의 샘플이 하나의 문장입니다. 하지만 문서 전체가 될 수도 있습니다)\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# 데이터에 있는 모든 토큰의 인덱스를 구축한다.\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    # split() 메서드를 사용해 샘플을 토큰으로 나눈다.\n",
    "    # 실전에서는 구둣점과 특수 문자도 사용합니다.\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            # 단어마다 고유한 인덱스를 할당한다.\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            # 인덱스 0은 사용하지 않는다.\n",
    "            \n",
    "# 샘플을 벡터로 변환\n",
    "# 각 샘플에서 max_length 까지 단어만 사용\n",
    "max_length = 10\n",
    "\n",
    "# 결가를 저장할 배열\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The': 1,\n",
       " 'cat': 2,\n",
       " 'sat': 3,\n",
       " 'on': 4,\n",
       " 'the': 5,\n",
       " 'mat.': 6,\n",
       " 'dog': 7,\n",
       " 'ate': 8,\n",
       " 'my': 9,\n",
       " 'homework.': 10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.shape : (2, 10, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('results.shape :', results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문자 수준 원-핫 인코딩(간단한 예)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable  # 출력 가능한 모든 아스키(ASCII) 문자\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values())+1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample[:max_length]):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36, 'A': 37, 'B': 38, 'C': 39, 'D': 40, 'E': 41, 'F': 42, 'G': 43, 'H': 44, 'I': 45, 'J': 46, 'K': 47, 'L': 48, 'M': 49, 'N': 50, 'O': 51, 'P': 52, 'Q': 53, 'R': 54, 'S': 55, 'T': 56, 'U': 57, 'V': 58, 'W': 59, 'X': 60, 'Y': 61, 'Z': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95, '\\t': 96, '\\n': 97, '\\r': 98, '\\x0b': 99, '\\x0c': 100}\n"
     ]
    }
   ],
   "source": [
    "print(token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.shape : (2, 50, 101)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('results.shape :', results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**케라스**에는 원본 텍스트 데이터를 단어 또는 문자 수준의 **원-핫 인코딩으로 변환해주는 유틸리티**가 있습니다. 특수 문자를 제거하거나 빈도가 높은 N개의 단어만을 선택(입력 벡터 공간이 너무 커지지 않도록 하기 위한 일반적인 제한 방법입니다)하는 등 여러 가지 중요한 기능들이 있기 때문에 이 유틸리티를 사용하는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 케라스를 사용한 단어 수준의 원-핫 인코딩: [[링크]](https://keras.io/preprocessing/text/) 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras \n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "word_index :\n",
      " {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# 가장 빈도가 높은 1,000개의 단어만 선택하도록 Tokenizer 객체를 만든다.\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "# 단어 인덱스를 구축한다.\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# 문자열을 정수 인덱스의 리스트로 변환\n",
    "# sequences = [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "# 직접 원-핫 이진 벡터 표현을 얻을 수 있다.\n",
    "# 원-핫 인코딩 외에 다른 벡터화 방법들도 제공한다.\n",
    "# mode : one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "# 계산된 단어 인덱스를 구한다.\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {:d} unique tokens.'.format(len(word_index)))\n",
    "print('word_index :\\n', word_index)\n",
    "one_hot_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩의 변종 중 하나는 **원-핫 해싱(one-hot hashing)** 기법입니다. \n",
    "- 이 방식은 어휘 사전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용합니다. \n",
    "- 각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 딕셔너리에 저장하는 대신에 **단어를 해싱하여 고정된 크기의 벡터로 변환**합니다. \n",
    "- 일반적으로 간단한 **해싱 함수**를 사용합니다. \n",
    "- 이 방식의 주요 장점은 **명시적인 단어 인덱스가 필요 없기 때문에 메모리를 절약하고 온라인 방식으로 데이터를 인코딩**할 수 있습니다(전체 데이터를 확인하지 않고 토큰을 생성할 수 있습니다). \n",
    "- 한 가지 단점은 **해시 충돌(hash collision)**입니다. \n",
    "    - 두 개의 단어가 같은 해시를 만들면 이를 바라보는 머신 러닝 모델은 단어 사이의 차이를 인식하지 못합니다. \n",
    "    - 해싱 공간의 차원이 해싱될 고유 토큰의 전체 개수보다 훨씬 크면 해시 충돌의 가능성은 감소합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해싱 기법을 사용한 단어 수준의 원-핫 인코딩(간단한 예):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# 단어를 크기가 1,000인 벡터로 저장\n",
    "# 1,000개(또는 그이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것이다.\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        # 단어를 해싱하여 0과 1,000 사이의 랜덤한 정수 인덱스로 변환한다.\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.shape : (2, 10, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('results.shape :', results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 케라스를 이용한 해싱 기법을 사용한 단어 수준의 원-핫 인코딩:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[615, 555, 806, 866, 615, 796], [615, 640, 168, 664, 402]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "\n",
    "dimensionality = 1000\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "hashing_results = [hashing_trick(sample, dimensionality) for sample in samples]\n",
    "hashing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence to matrix\n",
    "results = np.zeros((len(hashing_results), dimensionality))\n",
    "\n",
    "for i, hashing_result in enumerate(hashing_results):\n",
    "    results[i][hashing_result] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.shape : (2, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('results.shape :', results.shape)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 단어 임베딩 사용하기\n",
    "\n",
    "단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 **단어 임베딩**이라는 밀집 **단어 벡터**를 사용하는 것입니다. \n",
    "- 원-핫 인코딩으로 만든 벡터는 희소하고(대부분 0으로 채워짐) 고차원입니다(어휘 사전에 있는 단어의 수와 차원이 같습니다). \n",
    "- 반면 **단어 임베딩**은 **저차원의 실수형 벡터**입니다(희소 벡터의 반대인 밀집 벡터입니다). \n",
    "\n",
    "원-핫 인코딩으로 얻은 단어 벡터와 달리 **단어 임베딩은 데이터로부터 학습**됩니다. \n",
    "- 보통 256차원, 512차원 또는 큰 어휘 사전을 다룰 때는 1,024차원의 단어 임베딩을 사용합니다. \n",
    "- 반면 원-핫 인코딩은 (20,000개의 토큰으로 이루어진 어휘 사전을 만들려면) 20,000차원 또는 그 이상의 벡터일 경우가 많습니다. \n",
    "- 따라서 단어 임베딩이 더 많은 정보를 적은 차원에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Embedding` 레이어를 사용하여 단어 임베딩 학습하기\n",
    "\n",
    "단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 **랜덤하게 벡터를 선택**하는 것입니다. \n",
    "- 이 방식의 문제점은 임베딩 **공간이 구조적이지 않다는 것**입니다. \n",
    "- 예를 들어 'accurate'와 'exact' 단어가 대부분 문장에서 비슷한 의미로 사용되지만 완전히 다른 임베딩을 가지게 됩니다. \n",
    "- 심층 신경망이 이런 임의의 구조적이지 않은 임베딩 공간을 이해하기는 어렵습니다.\n",
    "\n",
    "단어 벡터 사이에 조금 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 **의미 관계를 반영**해야 합니다. \n",
    "\n",
    "단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다. \n",
    "- 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것입니다. \n",
    "- 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 **단어 사이의 의미 거리와 관계**되어 있습니다(멀리 떨어진 위치에 임베딩된 단어의 의미는 서로 다르고 반면 비슷한 단어들은 가까이 임베딩됩니다). \n",
    "- 거리외에 임베딩 공간의 특정 방향도 의미를 가질 수 있습니다.\n",
    "\n",
    "실제 단어 임베딩 공간에서 의미 있는 기하학적 변환의 일반적인 예는 '성별' 벡터와 '복수(plural)' 벡터입니다. \n",
    "- 예를 들어 'king' 벡터에 'female' 벡터를 더하면 'queen' 벡터가 됩니다. \n",
    "- 'plural' 벡터를 더하면 'kings'가 됩니다. \n",
    "- 단어 임베딩 공간은 전형적으로 이런 해석 가능하고 잠재적으로 유용한 수천 개의 벡터를 특성으로 가집니다.\n",
    "\n",
    "\n",
    "<img src=\"./images/w2v.png\" height=\"60%\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "사람의 언어를 완벽하게 매핑해서 어떤 자연어 처리 작업에도 사용할 수 있는 이상적인 단어 임베딩 공간이 있을까요? 아마도 가능하겠지만 아직까지 이런 종류의 공간은 만들지 못했습니다. 사람의 언어에도 그런 것은 없습니다. 세상에는 많은 다른 언어가 있고 언어는 특정 문화와 환경을 반영하기 때문에 서로 동일하지 않습니다. \n",
    "\n",
    "실제로 좋은 단어 임베딩 공간을 만드는 것은 문제에 따라 크게 달라집니다. \n",
    "- 영어로 된 영화 리뷰 감성 분석 모델을 위한 완벽한 단어 임베딩 공간은 영어로 된 법률 문서 분류 모델을 위한 완벽한 임베딩 공간과 다를 것 같습니다. \n",
    "- 특정 의미 관계의 중요성이 작업에 따라 다르기 때문입니다.\n",
    "\n",
    "따라서 **새로운 작업에는 새로운 임베딩**을 학습하는 것이 타당합니다. \n",
    "\n",
    "다행히 역전파를 사용해 쉽게 만들 수 있고 **케라스**를 사용하면 더 쉽습니다. [**Embedding 층**](https://keras.io/layers/embeddings/)의 가중치를 학습하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "# Embedding 레이어는 적어도 두 개의 매개변수를 받는다.\n",
    "# - 가능한 토큰의 개수 (size of vocabulary)\n",
    "# - 임베딩 차원\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding` 층을 (특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하는 것이 가장 좋습니다. 정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환합니다. 딕셔너리 탐색은 효율적으로 수행됩니다.\n",
    "- TensorFlow를 백엔드로 사용할 경우 `tf.nn.embedding_lookup()` 함수를 사용하여 병렬처리된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Word Index} \\rightarrow \\text{Embedding layer} \\rightarrow \\text{Corresponding word vector}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/embedding02.jpg\" height=\"60%\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding` 층은 크기가 `(samples, sequence_length)`인 2D 정수 텐서를 입력으로 받습니다. 각 샘플은 정수의 시퀀스입니다. 가변 길이의 시퀀스를 임베딩할 수 있습니다. \n",
    "- 예를 들어 위 예제의 `Embedding` 층에 `(32, 10)` 크기의 배치(길이가 10인 시퀀스 32개로 이루어진 배치)나 `(64, 15)` 크기의 배치(길이가 15인 시퀀스 64개로 이루어진 배치)를 주입할 수 있습니다. \n",
    "    - 배치에 있는 모든 시퀀스는 길이가 같아야 하므로(하나의 텐서에 담아야 하기 때문에) 작은 길이의 시퀀스는 0으로 패딩되고 길이가 더 긴 시퀀스는 잘립니다.\n",
    "\n",
    "`Embedding` 층은 크기가 `(samples, sequence_length, embedding_dimensionality)`인 3D 실수형 텐서를 반환합니다.\n",
    "- 이런 3D 텐서는 RNN 층이나 1D 합성곱 층에서 처리됩니다(둘 다 이어지는 절에서 소개하겠습니다).\n",
    "\n",
    "`Embedding` 층의 객체를 생성할 때 가중치(토큰 벡터를 위한 내부 딕셔너리)는 다른 층과 마찬가지로 **랜덤하게 초기화**됩니다. \n",
    "- 훈련하면서 이 단어 벡터는 역전파를 통해 점차 조정되어 이어지는 모델이 사용할 수 있도록 임베팅 공간을 구성합니다. \n",
    "- 훈련이 끝나면 임베딩 공간은 특정 문제에 특화된 구조를 많이 가지게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 익숙한 IMDB 영화 리뷰 감성 예측 문제에 적용해 보죠. \n",
    "- 먼저 데이터를 준비합니다. \n",
    "- 영화 리뷰에서 가장 빈도가 높은 10,000개의 단어를 추출하고(처음 이 데이터셋으로 작업했던 것과 동일합니다) 리뷰에서 20개 단어 이후는 버립니다. \n",
    "- 이 네트워크는 10,000개의 단어에 대해 8 차원의 임베딩을 학습하여 정수 시퀀스 입력(2D 정수 텐서)를 임베딩 시퀀스(3D 실수형 텐서)로 바꿀 것입니다. \n",
    "- 그 다음 이 텐서를 2D로 펼쳐서 분류를 위한 Dense 층을 훈련하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape (before transform) : (25000,)\n",
      "test_x's shape (before transform) : (25000,)\n",
      "train_x's shape (after transform) : (25000, 20)\n",
      "test_x's shape (after transform) : (25000, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# 특성으로 사용할 단어의 수\n",
    "max_features = 10000\n",
    "# 사용할 텍스트의 길이 (가장 빈번한 max_features 개의 단어만 사용합니다)\n",
    "maxlen = 20\n",
    "\n",
    "# 정수 리스트로 데이터를 로드한다.\n",
    "(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(\"train_x's shape (before transform) :\", train_x.shape)\n",
    "print(\"test_x's shape (before transform) :\", test_x.shape)\n",
    "\n",
    "# 리스트를 (samples, maxlen) 크기의 2D 정수 텐서로 변환한다\n",
    "train_x = preprocessing.sequence.pad_sequences(train_x, maxlen=maxlen)\n",
    "test_x = preprocessing.sequence.pad_sequences(test_x, maxlen=maxlen)\n",
    "\n",
    "print(\"train_x's shape (after transform) :\", train_x.shape)\n",
    "print(\"test_x's shape (after transform) :\", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 59us/step - loss: 0.6743 - acc: 0.6073 - val_loss: 0.6291 - val_acc: 0.6876\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.5511 - acc: 0.7482 - val_loss: 0.5302 - val_acc: 0.7230\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 0.4650 - acc: 0.7860 - val_loss: 0.5010 - val_acc: 0.7428\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.4224 - acc: 0.8083 - val_loss: 0.4942 - val_acc: 0.7490\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.3939 - acc: 0.8238 - val_loss: 0.4939 - val_acc: 0.7546\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.3702 - acc: 0.8373 - val_loss: 0.4964 - val_acc: 0.7542\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.3492 - acc: 0.8500 - val_loss: 0.5017 - val_acc: 0.7560\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.3300 - acc: 0.8601 - val_loss: 0.5075 - val_acc: 0.7570\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.3118 - acc: 0.8710 - val_loss: 0.5159 - val_acc: 0.7534\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.2944 - acc: 0.8787 - val_loss: 0.5261 - val_acc: 0.7522\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "# 나중에 임베딩된 입력을 Flatten 층에서 \n",
    "# 펼치기 위해 Embedding 층에 input_length를 지정한다.\n",
    "model.add(layers.Embedding(10000, 8, input_length=maxlen))\n",
    "# Embedding 층의 출력은 (samples, maxlen, 8)이 된다.\n",
    "\n",
    "# 3D 임베딩 텐서를 (samples, maxlen * 8) 형태의 2D 텐서로 펼친다.\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# 분류기를 추가한다.\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_x, train_y, \n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 75% 정도의 검증 정확도가 나옵니다. 리뷰에서 20개의 단어만 사용한 것치고 꽤 좋은 결과입니다. \n",
    "\n",
    "하지만 임베딩 시퀀스를 펼치고 하나의 `Dense` 층을 훈련했으므로 입력 시퀀스에 있는 **각 단어를 독립적**으로 다루었습니다. 단어 사이의 **관계나 문장의 구조를 고려하지 않았습니다**(예를 들어 이 모델은 “this movie is a bomb”와 “this movie is the bomb”를 부정적인 리뷰로 동일하게 다룰 것입니다). \n",
    "\n",
    "각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 **순환 층**이나 **1D 합성곱 층**을 추가하는 것이 좋습니다. 다음 절에서 이에 관해 집중적으로 다루겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding 결과 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1_input (InputLaye (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "=================================================================\n",
      "Total params: 80,000\n",
      "Trainable params: 80,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_output = model.layers[0].output\n",
    "\n",
    "embedding_model = models.Model(inputs=model.input, outputs=embedding_output)\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "sample1 = train_x[0]\n",
    "sample1_to_text = [index_to_word[idx] for idx in sample1]\n",
    "\n",
    "sample2 = train_x[100]\n",
    "sample2_to_text = [index_to_word[idx] for idx in sample2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1:\n",
      "[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n",
      "sample1_to_text:\n",
      "['their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'heartfelt', 'film', 'want', 'an']\n"
     ]
    }
   ],
   "source": [
    "print('sample1:\\n{}'.format(sample1))\n",
    "print('sample1_to_text:\\n{}'.format(sample1_to_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sample1 = embedding_model.predict(sample1.reshape([1,20]))\n",
    "embedding_sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
