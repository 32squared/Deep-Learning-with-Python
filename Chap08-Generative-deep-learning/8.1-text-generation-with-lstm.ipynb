{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 LSTM으로 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 시퀀스 데이터를 어떻게 생성할까?\n",
    "\n",
    "\n",
    "- 딥러닝에서 시퀀스 데이터를 생성하는 일반적인 방법은 이전 토큰을 입력으로 사용해서 시퀀스의 다음 1개 또는 몇 개의 토큰을 RNN이나 ConvNet으로 예측하는 것이다.\n",
    "\n",
    "    - 예를들어 \"the cat is on ma\"란 입력이 주어지면 다음 글자인 타깃 \"t\"를 예측하도록 네트워크를 훈련한다.\n",
    "    \n",
    "\n",
    "- 텍스트 데이터를 다룰 때 토큰은 보통 단어 또는 글자이다.\n",
    "\n",
    "- 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모델링할 수 있는 네트워크를 **언어 모델**(language model)이라고 한다.\n",
    "\n",
    "    - 언어 모델을 언어의 통계적 구조인 잠재공간(latent space)을 탐색한다.\n",
    "    \n",
    "    \n",
    "- 언어 모델을 훈련하고 나면 이 모델에서 샘플링 할 수 있으며, 이를 통해 새로운 시퀀스를 생성한다.\n",
    "    \n",
    "    1. 초기 텍스트 문자열(**조건 데이터**(conditioning data))을 주입하고 새로운 글자나 단어를 생성한다. \n",
    "    2. 생성된 출력은 다시 입력 데이터로 추가된다.\n",
    "    3. `1~2`과정을 여러번 반복한다.\n",
    "    \n",
    "    \n",
    "![](./images/text-gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 샘플링 전략의 중요성\n",
    "\n",
    "- 텍스트를 생성할 때 다음 글자를 선택하는 방법이 중요하다.\n",
    "\n",
    "\n",
    "- 단순한 방법은 가장 높은 확률을 가진 글자를 선택하는 **탐욕적 샘플링**(greedy sampling)이다. \n",
    "    - 이 방법은 반복적이고 예상 가능한 문자열을 마들기 때문에 논리적인 언어처럼 보이지 않는다.\n",
    "    \n",
    "   \n",
    "- 다음 글자의 확률 분포에서 샘플링하는 과정에 무작위성을 추가하는 방법이 있는데, 이를 **확률적 샘플링**(stochastic sampling)이라고 부른다. \n",
    "    - 예를들어, `'e'`가 다음 글자가 될 확률이 `0.3`이라면, 모델이 `30%`정도는 이 글자를 선택한다.\n",
    "    \n",
    "    \n",
    "- 모델의 소프트맥스(softmax) 출력은 확률적 샘플링에 사용하기 좋다.\n",
    "    - 이따금 샘플링될 것 같지 않은 글자를 샘플링한다.\n",
    "    - 훈련 데이터에는 없지만 실제 같은 새로운 단어를 만들어 문장을 생성한다.\n",
    "    - 한가지 문제는, 샘플링 과정에서 무작위성의 양을 조절할 방법이 없는 것이다.\n",
    "    \n",
    "   \n",
    "- 생성 모델에서 샘플링을 할 때 생성 과정에서 무작위성의 양을 바꾸어 시도해 보는 것이 좋다.\n",
    "\n",
    "\n",
    "- 샘플링 과정에서 확률의 양을 조절하기 위해 **소프트맥스 온도**(softmax temperature)라는 파라미터를 사용한다.\n",
    "    - 이 파라미터는 샘플링에 사용되는 확률 분포의 엔트로피를 나타낸다. \n",
    "    - `temperature` 값이 주어지면 가중치를 적용하여 모델의 소프트맥스 출력인 원본 확률 분포에서 새로운 확률 분포를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "dist = np.random.randint(1, 10, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plt_dist = (\n",
    "    reweight_distribution(dist, temperature=0.01),\n",
    "    reweight_distribution(dist, temperature=0.2),\n",
    "    reweight_distribution(dist, temperature=0.4),\n",
    "    reweight_distribution(dist, temperature=0.6),\n",
    "    reweight_distribution(dist, temperature=0.8),\n",
    "    reweight_distribution(dist, temperature=1.0)\n",
    ")\n",
    "\n",
    "plt_dist = np.column_stack(plt_dist)\n",
    "\n",
    "df = pd.DataFrame(plt_dist, \n",
    "                  columns=['0.01', '0.2', '0.4','0.6', '0.8', '1.0'])\n",
    "\n",
    "df.hist(alpha=0.5, bins=10, figsize=(12, 5), sharex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 글자 수준의 LSTM 텍스트 생성 모델 구현\n",
    "\n",
    "이런 아이디어를 케라스로 구현해 보죠. 먼저 언어 모델을 학습하기 위해 많은 텍스트 데이터가 필요합니다. 위키피디아나 반지의 제왕처럼 아주 큰 텍스트 파일이나 텍스트 파일의 묶음을 사용할 수 있습니다. \n",
    "\n",
    "이 예에서는 19세기 후반 독일의 철학자 니체의 글을 사용하겠습니다(영어로 번역된 글입니다). 학습할 언어 모델은 일반적인 영어 모델이 아니라 니체의 문체와 특정 주제를 따르는 모델일 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리\n",
    "\n",
    "먼저 말뭉치를 다운로드하고 소문자로 바꿉니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
    ")\n",
    "\n",
    "text = open(path).read().lower()\n",
    "print('말뭉치 크기:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 `maxlen` 길이를 가진 시퀀스를 중복하여 추출합니다. 추출된 시퀀스를 원-핫 인코딩으로 변환하고 크기가 `(sequences, maxlen, unique_characters)`인 3D 넘파이 배열 `x`로 합칩니다. 동시에 훈련 샘플에 상응하는 타깃을 담은 배열 y를 준비합니다. 타깃은 추출된 시퀀스 다음에 오는 원-핫 인코딩된 글자입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 개수: 200278\n",
      "sentences[0]\n",
      " preface\n",
      "\n",
      "\n",
      "supposing that truth is a woman--what then? is the\n",
      "==================================================\n",
      "text[:60]\n",
      " preface\n",
      "\n",
      "\n",
      "supposing that truth is a woman--what then? is the\n"
     ]
    }
   ],
   "source": [
    "# 60개 글자로 된 시퀀스를 추출한다.\n",
    "maxlen = 60\n",
    "\n",
    "# 세 글자씩 건너 뛰면서 새로운 시퀀스를 샘플링한다.\n",
    "step = 3\n",
    "\n",
    "# 추출한 시퀀스를 담을 리스트\n",
    "sentences = []\n",
    "\n",
    "# 타깃(시퀀스 다음 글자)을 담을 리스트\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i+maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('시퀀스 개수:', len(sentences))\n",
    "print('sentences[0]\\n', sentences[0])\n",
    "print('='*50)\n",
    "print('text[:60]\\n', text[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유한 글자: 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.']\n"
     ]
    }
   ],
   "source": [
    "# 말뭉치에서 고유한 글자를 담은 리스트\n",
    "chars = sorted(list(set(text)))\n",
    "print('고유한 글자:', len(chars))\n",
    "print(chars[:10])\n",
    "# chars 리스트에 있는 글자와 글자의 인덱스를 매핑한 딕셔너리\n",
    "# char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "char_indices = {char: idx for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터화...\n",
      "x.shape : (200278, 60, 57)\n",
      "y.shape : (200278, 57)\n"
     ]
    }
   ],
   "source": [
    "# 글자를 원-핫 인코딩하여 0과 1의 이진 배열로 바꾼다.\n",
    "print('벡터화...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentences in enumerate(sentences):\n",
    "    for t, char in enumerate(sentences):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print('x.shape :', x.shape)\n",
    "print('y.shape :', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네트워크 구성\n",
    "\n",
    "이 네트워크는 하나의 LSTM 층과 그 뒤에 Dense 분류기가 뒤따릅니다. 분류기는 가능한 모든 글자에 대한 소프트맥스 출력을 만듭니다. 순환 신경망이 시퀀스 데이터를 생성하는 유일한 방법은 아닙니다. \n",
    "\n",
    "최근에는 1D 컨브넷도 이런 작업에 아주 잘 들어 맞는다는 것이 밝혀졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "타깃이 원-핫 인코딩되어 있기 때문에 모델을 훈련하기 위해 categorical_crossentropy 손실을 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언어 모델 훈련과 샘플링\n",
    "\n",
    "훈련된 모델과 시드로 쓰일 간단한 텍스트가 주어지면 다음과 같이 반복하여 새로운 텍스트를 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "1. 지금까지 생성된 텍스트를 주입하여 모델에서 다음 글자에 대한 확률 분포를 뽑습니다.\n",
    "2. 특정 온도로 이 확률 분포의 가중치를 조정합니다.\n",
    "3. 가중치가 조정된 분포에서 무작위로 새로운 글자를 샘플링합니다.\n",
    "4. 새로운 글자를 생성된 텍스트의 끝에 추가합니다.\n",
    "\n",
    "\n",
    "다음 코드는 모델에서 나온 원본 확률 분포의 가중치를 조정하고 새로운 글자의 인덱스를 추출합니다(샘플링 함수입니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 다음 반복문은 반복적으로 훈련하고 텍스트를 생성합니다. 에포크마다 학습이 끝난 후 여러가지 온도를 사용해 텍스트를 생성합니다. 이렇게 하면 모델이 수렴하면서 생성된 텍스트가 어떻게 진화하는지 볼 수 있습니다. 온도가 샘플링 전략에 미치는 영향도 보여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 : 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 61s 302us/step - loss: 1.6068\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the science of the spirit in the sensitions of the sense of the sense the sense of the strength the sentiment to the sensitions of the self-contries to the belare the contrary the sense and the sense of the sense of the sense of the sense of the sension of the sense of the sense the sense of the sense of the explainty the sense of the sense of the sense of the against the sense and the sense of \n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the seedly the sense his explainted the explantial sense in the fellowment of the very precised to the seamed the intential creations such a conscience and does the came into an it\n",
      "it is indeed his all the seems and lough the enory and for his development the contemplate is all such the sentiment of its own all the contraint and in the exactional fally of the good the soul and part of the sensiv\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for their exmising are wneach conscience of discontace and comorised a geavate does man to bring under bundactured at the exactions of the upon a wild not have parable altesled for no life on mind been nut in mint irmany from senses of himself nather, and anywather the words: an is all inthill,\n",
      "from suld such have with the strength it\" hid is\n",
      "spirit be\n",
      "one worbious tooles, shan\n",
      "the relang but the gr\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through formicatic iu clading and fortadayes! fone ca idstesianing \"be own gro isfortthing their\n",
      "past europe than howwmen altheacy vueblies have alone, q;ellectuent, he nin lioking caliary frame other actaver form of the much such who don\n",
      "tapter the free oun towey back man, purence strengines\n",
      "contophimate the fexmole but theyafily orn, for aways ditgange, leadny in\n",
      "t fines beings hand onough, the stone unre\n",
      "에포크 : 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 60s 302us/step - loss: 1.5270\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the senses to the supposine in the soul a desion of the supposine of the supposing of the supporined to the such a sought and the strength, who consequence of the superiority and the strength, and the all the supposine of the supposine of the superioning to the superiority of the distingule of the supposine of the superious and precisely to have all the strength, and the soul and the supposine o\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for not one and soul who was the could be which has personated and soul and sought to mean to the instates the more of menally merely himself, and from the perceive and percained like the all the is one of the hopes which the superiorations, which the superiority and the still and distances of the spirits of the the same to him of the understand as this the altered of the love of the grough and him \n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the matich of eher sprinest of what gives all their musicitus\n",
      "decemand.\n",
      "\n",
      "things, the interesting because or estimed underederss truth ougners to lose he is nothing all\n",
      "exeller, afternation of himself himself the philosophers, what deterpratical,s himbelusity\n",
      "of themee ma, and pestedolity means trimpleners sciences, as i is not: the\n",
      "soul for the are antiet of the gained vid\n",
      "upon the\n",
      "still,\n",
      "they k\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for imrelfesty in what shordred, prevention for itselfly hemely sy extences austed not not one wall\" utians of\n",
      "accorring nhightec imbrloft andaothers\n",
      "she\n",
      "lathers. why\n",
      "platerism, under the lual.\n",
      "\n",
      "  monaic for exusioe are the louge parteculaally. butsitied, se\n",
      "tovens; which his enura gregagic where ha\n",
      "with he advanated\n",
      "one loughtusioning inventance of usoness hfeneth. a disperst, and all his go really\n",
      "에포크 : 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 61s 302us/step - loss: 1.4855\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the same all the man the the belief and the strong and the all in the same a proved the all the same the morality of a more the same the same the present the strong the has the the as a man in the same in the same all the present the same the same all the german and the and the same and the strong the same all the actions of the do in the sentement of the faith of the south the same should be a \n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for every conscience the be the be man in the hellever be inspire and be the communition of the values its\n",
      "strong that the be and characteristic then in we lork profound be the same according to be the predelf, the contemple of the prede the artisticse when it is all which the induce probably in what is it is not all the man and most because but the has a distrust that a man or and its slaver state \n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through fore induce it we mettologi; oncablarism, is noble\n",
      "minard, in the expracticting \"instincts), erotest hypochor percemse style: shereit to\n",
      "part the sturely kinds mither condition south two punry the most jesome our too\n",
      "take it, or of the crace, thinkings--ulportance action, more loor attain from man,\n",
      "this\n",
      "sutivy to developing (sturchan regamishing which, look undesed--atsomithes. that christian, thoug\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through footh autedably! it is it, who god a callite with them nootheues. the pacticl hasister up of those lovers\n",
      "then usesteminism. in whates stitures me been ne courisfy us is bein be whom ajust\n",
      "posseblety, to pernicand\n",
      "hamped the\n",
      "been veses unplect pener to\n",
      "be peted eno's charm tole\n",
      "to cipulawing wor sked wodalege estement\n",
      "men, us permangers, and consciounthe hasistry, cownthed thus, \n",
      "\"tomend in view,\n",
      "no\n",
      "에포크 : 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 60s 301us/step - loss: 1.4583\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the sense the character of the same that the sense of the senses and the man is the sense and the accordance of the consequent and consequently and the sense and the man of the strongest that the senses of the sense and stronger and deal of the sense of the sense and condemned the presing and consequent and stronger of the sense and stronger and discover to the senses of the sense of the sense o\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for himself to the most constince, and mean, and something and foreary of the pitaly to the accessity of politics of the science of the sensess of the parts of the constant consequently has a stronge to self-complexity in man of the prite morality of the delicate grace of the foreignt that the sourhts of the strongest concerning the man who personal earlient and be to make an the soul in the same wi\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for only happences the sampt\n",
      "and inclination with all--character, nowadifocked ebstianly is\n",
      "europe with a pussible an oriently seciation\n",
      "of disguyitohele\n",
      "man have been divine truth ackionary things, may knows him intricc in the valuesment and in are because only they crartcabutity, which heared to much which ctapte\" from he pertains, however norrolities to aspottesics and motive it i misunwints and \n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for theny day,\n",
      "love is be will have volace of the olnew ideaity of womp what nearart!\n",
      "for breadly to tailsv'smole,\n",
      "as werm in\n",
      "them. the armity lible; feeptrands, not.s\n",
      "vent noist should it verior\n",
      "imprispts of geemnets, the impurres1ly retrogr would think of recodful\"s affaraces on inperces this accessity of etts by the souch\n",
      "yeaftoour\n",
      "lourvely an acculting to\n",
      "mackat-a it old to haboissvely; thie she\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "random.seed(42)\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "# 5 epoch 동안 모델을 훈련한다.\n",
    "for epoch in range(1, 5):\n",
    "    print('에포크 :', epoch)\n",
    "    # 데이터에서 한 번만 반복해서 모델을 학습한다.\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    \n",
    "    # 무작위로 시드 텍스트를 선택한다\n",
    "    seed_text = text[start_index: start_index + maxlen]\n",
    "    print('--- 시드 텍스트:\"' + seed_text + '\"')\n",
    "    \n",
    "    # 여러가지 샘플링 온도를 시도한다.\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ 온도:', temperature)\n",
    "        generated_text = seed_text\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # 시드 텍스트에서 시작해서 400개의 글자를 생성한다.\n",
    "        for i in range(400):\n",
    "            # 지금까지 생성된 글자를 원-핫 인코딩으로 바꾼다.\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            # 다음 글자를 샘플링 한다.\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과확인\n",
    "\n",
    "- 위의 결과에서도 확인할 수 있듯이 낮은 온도(`temperature`)는 아주 반복적이고 예상되는 텍스트를 만든다.\n",
    "    - 하지만 국부적인 구조는 실제와 매우 같다. \n",
    "    - 특히 모든 단어들이 실제 영단어다.\n",
    "    \n",
    "    \n",
    "- 높은 온도에서 생성된 텍스트는 창의적인 텍스트들이 생성된다.\n",
    "    - 실제 단어가 아닌 실제 단어와 비슷하게 보이는 단어들을 생성해낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 정리\n",
    "\n",
    "- 이전의 토큰이 주어지면 다음 토큰(들)을 예측하는 모델을 훈련하여 시퀀스 데이터를 생성할 수 있다.\n",
    "\n",
    "- 텍스트의 경우 이런 모델을 **언어 모델**이라고 부른다. \n",
    "\n",
    "- 다음 토큰을 샘플링할 때 모델이 만든 출력에 집중하는 것과 무작위성을 주입하는 것 사이에 균형을 맞추어야 한다.\n",
    "\n",
    "- 이를 위해 **소프트맥스 온도** 개념을 사용한다. 항상 다양한 온도를 실험해서 적절한 값을 찾아야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
