{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 LSTM으로 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 시퀀스 데이터를 어떻게 생성할까?\n",
    "\n",
    "\n",
    "- 딥러닝에서 시퀀스 데이터를 생성하는 일반적인 방법은 이전 토큰을 입력으로 사용해서 시퀀스의 다음 1개 또는 몇 개의 토큰을 RNN이나 ConvNet으로 예측하는 것이다.\n",
    "\n",
    "    - 예를들어 \"the cat is on ma\"란 입력이 주어지면 다음 글자인 타깃 \"t\"를 예측하도록 네트워크를 훈련한다.\n",
    "    \n",
    "\n",
    "- 텍스트 데이터를 다룰 때 토큰은 보통 단어 또는 글자이다.\n",
    "\n",
    "- 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모델링할 수 있는 네트워크를 **언어 모델**(language model)이라고 한다.\n",
    "\n",
    "    - 언어 모델을 언어의 통계적 구조인 잠재공간(latent space)을 탐색한다.\n",
    "    \n",
    "    \n",
    "- 언어 모델을 훈련하고 나면 이 모델에서 샘플링 할 수 있으며, 이를 통해 새로운 시퀀스를 생성한다.\n",
    "    \n",
    "    1. 초기 텍스트 문자열(**조건 데이터**(conditioning data))을 주입하고 새로운 글자나 단어를 생성한다. \n",
    "    2. 생성된 출력은 다시 입력 데이터로 추가된다.\n",
    "    3. `1~2`과정을 여러번 반복한다.\n",
    "    \n",
    "    \n",
    "![](./images/text-gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 샘플링 전략의 중요성\n",
    "\n",
    "- 텍스트를 생성할 때 다음 글자를 선택하는 방법이 중요하다.\n",
    "\n",
    "\n",
    "- 단순한 방법은 가장 높은 확률을 가진 글자를 선택하는 **탐욕적 샘플링**(greedy sampling)이다. \n",
    "    - 이 방법은 반복적이고 예상 가능한 문자열을 마들기 때문에 논리적인 언어처럼 보이지 않는다.\n",
    "    \n",
    "   \n",
    "- 다음 글자의 확률 분포에서 샘플링하는 과정에 무작위성을 추가하는 방법이 있는데, 이를 **확률적 샘플링**(stochastic sampling)이라고 부른다. \n",
    "    - 예를들어, `'e'`가 다음 글자가 될 확률이 `0.3`이라면, 모델이 `30%`정도는 이 글자를 선택한다.\n",
    "    \n",
    "    \n",
    "- 모델의 소프트맥스(softmax) 출력은 확률적 샘플링에 사용하기 좋다.\n",
    "    - 이따금 샘플링될 것 같지 않은 글자를 샘플링한다.\n",
    "    - 훈련 데이터에는 없지만 실제 같은 새로운 단어를 만들어 문장을 생성한다.\n",
    "    - 한가지 문제는, 샘플링 과정에서 무작위성의 양을 조절할 방법이 없는 것이다.\n",
    "    \n",
    "   \n",
    "- 생성 모델에서 샘플링을 할 때 생성 과정에서 무작위성의 양을 바꾸어 시도해 보는 것이 좋다.\n",
    "\n",
    "\n",
    "- 샘플링 과정에서 확률의 양을 조절하기 위해 **소프트맥스 온도**(softmax temperature)라는 파라미터를 사용한다.\n",
    "    - 이 파라미터는 샘플링에 사용되는 확률 분포의 엔트로피를 나타낸다. \n",
    "    - `temperature` 값이 주어지면 가중치를 적용하여 모델의 소프트맥스 출력인 원본 확률 분포에서 새로운 확률 분포를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\n",
    "    distribution = np.log(original_distribution) / temperature\n",
    "    distribution = np.exp(distribution)\n",
    "    return distribution / np.sum(distribution)\n",
    "\n",
    "dist = np.random.randint(1, 10, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "plt_dist = (\n",
    "    reweight_distribution(dist, temperature=0.01),\n",
    "    reweight_distribution(dist, temperature=0.2),\n",
    "    reweight_distribution(dist, temperature=0.4),\n",
    "    reweight_distribution(dist, temperature=0.6),\n",
    "    reweight_distribution(dist, temperature=0.8),\n",
    "    reweight_distribution(dist, temperature=1.0)\n",
    ")\n",
    "\n",
    "plt_dist = np.column_stack(plt_dist)\n",
    "\n",
    "df = pd.DataFrame(plt_dist, \n",
    "                  columns=['0.01', '0.2', '0.4','0.6', '0.8', '1.0'])\n",
    "\n",
    "df.hist(alpha=0.5, bins=10, figsize=(12, 5), sharex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.4 글자 수준의 LSTM 텍스트 생성 모델 구현\n",
    "\n",
    "이런 아이디어를 케라스로 구현해 보죠. 먼저 언어 모델을 학습하기 위해 많은 텍스트 데이터가 필요합니다. 위키피디아나 반지의 제왕처럼 아주 큰 텍스트 파일이나 텍스트 파일의 묶음을 사용할 수 있습니다. \n",
    "\n",
    "이 예에서는 19세기 후반 독일의 철학자 니체의 글을 사용하겠습니다(영어로 번역된 글입니다). 학습할 언어 모델은 일반적인 영어 모델이 아니라 니체의 문체와 특정 주제를 따르는 모델일 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리\n",
    "\n",
    "먼저 말뭉치를 다운로드하고 소문자로 바꿉니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
    ")\n",
    "\n",
    "text = open(path).read().lower()\n",
    "print('말뭉치 크기:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 `maxlen` 길이를 가진 시퀀스를 중복하여 추출합니다. 추출된 시퀀스를 원-핫 인코딩으로 변환하고 크기가 `(sequences, maxlen, unique_characters)`인 3D 넘파이 배열 `x`로 합칩니다. 동시에 훈련 샘플에 상응하는 타깃을 담은 배열 y를 준비합니다. 타깃은 추출된 시퀀스 다음에 오는 원-핫 인코딩된 글자입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 개수: 200278\n",
      "sentences[0]\n",
      " preface\n",
      "\n",
      "\n",
      "supposing that truth is a woman--what then? is the\n",
      "==================================================\n",
      "text[:60]\n",
      " preface\n",
      "\n",
      "\n",
      "supposing that truth is a woman--what then? is the\n"
     ]
    }
   ],
   "source": [
    "# 60개 글자로 된 시퀀스를 추출한다.\n",
    "maxlen = 60\n",
    "\n",
    "# 세 글자씩 건너 뛰면서 새로운 시퀀스를 샘플링한다.\n",
    "step = 3\n",
    "\n",
    "# 추출한 시퀀스를 담을 리스트\n",
    "sentences = []\n",
    "\n",
    "# 타깃(시퀀스 다음 글자)을 담을 리스트\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i+maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('시퀀스 개수:', len(sentences))\n",
    "print('sentences[0]\\n', sentences[0])\n",
    "print('='*50)\n",
    "print('text[:60]\\n', text[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유한 글자: 57\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.']\n"
     ]
    }
   ],
   "source": [
    "# 말뭉치에서 고유한 글자를 담은 리스트\n",
    "chars = sorted(list(set(text)))\n",
    "print('고유한 글자:', len(chars))\n",
    "print(chars[:10])\n",
    "# chars 리스트에 있는 글자와 글자의 인덱스를 매핑한 딕셔너리\n",
    "# char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "char_indices = {char: idx for idx, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터화...\n",
      "x.shape : (200278, 60, 57)\n",
      "y.shape : (200278, 57)\n"
     ]
    }
   ],
   "source": [
    "# 글자를 원-핫 인코딩하여 0과 1의 이진 배열로 바꾼다.\n",
    "print('벡터화...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentences in enumerate(sentences):\n",
    "    for t, char in enumerate(sentences):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print('x.shape :', x.shape)\n",
    "print('y.shape :', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네트워크 구성\n",
    "\n",
    "이 네트워크는 하나의 LSTM 층과 그 뒤에 Dense 분류기가 뒤따릅니다. 분류기는 가능한 모든 글자에 대한 소프트맥스 출력을 만듭니다. 순환 신경망이 시퀀스 데이터를 생성하는 유일한 방법은 아닙니다. \n",
    "\n",
    "최근에는 1D 컨브넷도 이런 작업에 아주 잘 들어 맞는다는 것이 밝혀졌습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "타깃이 원-핫 인코딩되어 있기 때문에 모델을 훈련하기 위해 categorical_crossentropy 손실을 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언어 모델 훈련과 샘플링\n",
    "\n",
    "훈련된 모델과 시드로 쓰일 간단한 텍스트가 주어지면 다음과 같이 반복하여 새로운 텍스트를 생성할 수 있습니다.\n",
    "\n",
    "\n",
    "1. 지금까지 생성된 텍스트를 주입하여 모델에서 다음 글자에 대한 확률 분포를 뽑습니다.\n",
    "2. 특정 온도로 이 확률 분포의 가중치를 조정합니다.\n",
    "3. 가중치가 조정된 분포에서 무작위로 새로운 글자를 샘플링합니다.\n",
    "4. 새로운 글자를 생성된 텍스트의 끝에 추가합니다.\n",
    "\n",
    "\n",
    "다음 코드는 모델에서 나온 원본 확률 분포의 가중치를 조정하고 새로운 글자의 인덱스를 추출합니다(샘플링 함수입니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 다음 반복문은 반복적으로 훈련하고 텍스트를 생성합니다. 에포크마다 학습이 끝난 후 여러가지 온도를 사용해 텍스트를 생성합니다. 이렇게 하면 모델이 수렴하면서 생성된 텍스트가 어떻게 진화하는지 볼 수 있습니다. 온도가 샘플링 전략에 미치는 영향도 보여 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 : 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 61s 302us/step - loss: 1.6068\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the science of the spirit in the sensitions of the sense of the sense the sense of the strength the sentiment to the sensitions of the self-contries to the belare the contrary the sense and the sense of the sense of the sense of the sense of the sension of the sense of the sense the sense of the sense of the explainty the sense of the sense of the sense of the against the sense and the sense of \n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the seedly the sense his explainted the explantial sense in the fellowment of the very precised to the seamed the intential creations such a conscience and does the came into an it\n",
      "it is indeed his all the seems and lough the enory and for his development the contemplate is all such the sentiment of its own all the contraint and in the exactional fally of the good the soul and part of the sensiv\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for their exmising are wneach conscience of discontace and comorised a geavate does man to bring under bundactured at the exactions of the upon a wild not have parable altesled for no life on mind been nut in mint irmany from senses of himself nather, and anywather the words: an is all inthill,\n",
      "from suld such have with the strength it\" hid is\n",
      "spirit be\n",
      "one worbious tooles, shan\n",
      "the relang but the gr\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through formicatic iu clading and fortadayes! fone ca idstesianing \"be own gro isfortthing their\n",
      "past europe than howwmen altheacy vueblies have alone, q;ellectuent, he nin lioking caliary frame other actaver form of the much such who don\n",
      "tapter the free oun towey back man, purence strengines\n",
      "contophimate the fexmole but theyafily orn, for aways ditgange, leadny in\n",
      "t fines beings hand onough, the stone unre\n",
      "에포크 : 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 60s 302us/step - loss: 1.5270\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the senses to the supposine in the soul a desion of the supposine of the supposing of the supporined to the such a sought and the strength, who consequence of the superiority and the strength, and the all the supposine of the supposine of the superioning to the superiority of the distingule of the supposine of the superious and precisely to have all the strength, and the soul and the supposine o\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for not one and soul who was the could be which has personated and soul and sought to mean to the instates the more of menally merely himself, and from the perceive and percained like the all the is one of the hopes which the superiorations, which the superiority and the still and distances of the spirits of the the same to him of the understand as this the altered of the love of the grough and him \n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the matich of eher sprinest of what gives all their musicitus\n",
      "decemand.\n",
      "\n",
      "things, the interesting because or estimed underederss truth ougners to lose he is nothing all\n",
      "exeller, afternation of himself himself the philosophers, what deterpratical,s himbelusity\n",
      "of themee ma, and pestedolity means trimpleners sciences, as i is not: the\n",
      "soul for the are antiet of the gained vid\n",
      "upon the\n",
      "still,\n",
      "they k\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for imrelfesty in what shordred, prevention for itselfly hemely sy extences austed not not one wall\" utians of\n",
      "accorring nhightec imbrloft andaothers\n",
      "she\n",
      "lathers. why\n",
      "platerism, under the lual.\n",
      "\n",
      "  monaic for exusioe are the louge parteculaally. butsitied, se\n",
      "tovens; which his enura gregagic where ha\n",
      "with he advanated\n",
      "one loughtusioning inventance of usoness hfeneth. a disperst, and all his go really\n",
      "에포크 : 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 61s 302us/step - loss: 1.4855\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the same all the man the the belief and the strong and the all in the same a proved the all the same the morality of a more the same the same the present the strong the has the the as a man in the same in the same all the present the same the same all the german and the and the same and the strong the same all the actions of the do in the sentement of the faith of the south the same should be a \n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for every conscience the be the be man in the hellever be inspire and be the communition of the values its\n",
      "strong that the be and characteristic then in we lork profound be the same according to be the predelf, the contemple of the prede the artisticse when it is all which the induce probably in what is it is not all the man and most because but the has a distrust that a man or and its slaver state \n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through fore induce it we mettologi; oncablarism, is noble\n",
      "minard, in the expracticting \"instincts), erotest hypochor percemse style: shereit to\n",
      "part the sturely kinds mither condition south two punry the most jesome our too\n",
      "take it, or of the crace, thinkings--ulportance action, more loor attain from man,\n",
      "this\n",
      "sutivy to developing (sturchan regamishing which, look undesed--atsomithes. that christian, thoug\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through footh autedably! it is it, who god a callite with them nootheues. the pacticl hasister up of those lovers\n",
      "then usesteminism. in whates stitures me been ne courisfy us is bein be whom ajust\n",
      "posseblety, to pernicand\n",
      "hamped the\n",
      "been veses unplect pener to\n",
      "be peted eno's charm tole\n",
      "to cipulawing wor sked wodalege estement\n",
      "men, us permangers, and consciounthe hasistry, cownthed thus, \n",
      "\"tomend in view,\n",
      "no\n",
      "에포크 : 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 60s 301us/step - loss: 1.4583\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the sense the character of the same that the sense of the senses and the man is the sense and the accordance of the consequent and consequently and the sense and the man of the strongest that the senses of the sense and stronger and deal of the sense of the sense and condemned the presing and consequent and stronger of the sense and stronger and discover to the senses of the sense of the sense o\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for himself to the most constince, and mean, and something and foreary of the pitaly to the accessity of politics of the science of the sensess of the parts of the constant consequently has a stronge to self-complexity in man of the prite morality of the delicate grace of the foreignt that the sourhts of the strongest concerning the man who personal earlient and be to make an the soul in the same wi\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for only happences the sampt\n",
      "and inclination with all--character, nowadifocked ebstianly is\n",
      "europe with a pussible an oriently seciation\n",
      "of disguyitohele\n",
      "man have been divine truth ackionary things, may knows him intricc in the valuesment and in are because only they crartcabutity, which heared to much which ctapte\" from he pertains, however norrolities to aspottesics and motive it i misunwints and \n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for theny day,\n",
      "love is be will have volace of the olnew ideaity of womp what nearart!\n",
      "for breadly to tailsv'smole,\n",
      "as werm in\n",
      "them. the armity lible; feeptrands, not.s\n",
      "vent noist should it verior\n",
      "imprispts of geemnets, the impurres1ly retrogr would think of recodful\"s affaraces on inperces this accessity of etts by the souch\n",
      "yeaftoour\n",
      "lourvely an acculting to\n",
      "mackat-a it old to haboissvely; thie she\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "random.seed(42)\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "# 5 epoch 동안 모델을 훈련한다.\n",
    "for epoch in range(1, 5):\n",
    "    print('에포크 :', epoch)\n",
    "    # 데이터에서 한 번만 반복해서 모델을 학습한다.\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    \n",
    "    # 무작위로 시드 텍스트를 선택한다\n",
    "    seed_text = text[start_index: start_index + maxlen]\n",
    "    print('--- 시드 텍스트:\"' + seed_text + '\"')\n",
    "    \n",
    "    # 여러가지 샘플링 온도를 시도한다.\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ 온도:', temperature)\n",
    "        generated_text = seed_text\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # 시드 텍스트에서 시작해서 400개의 글자를 생성한다.\n",
    "        for i in range(400):\n",
    "            # 지금까지 생성된 글자를 원-핫 인코딩으로 바꾼다.\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            # 다음 글자를 샘플링 한다.\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결과확인\n",
    "\n",
    "- 위의 결과에서도 확인할 수 있듯이 낮은 온도(`temperature`)는 아주 반복적이고 예상되는 텍스트를 만든다.\n",
    "    - 하지만 국부적인 구조는 실제와 매우 같다. \n",
    "    - 특히 모든 단어들이 실제 영단어다.\n",
    "    \n",
    "    \n",
    "- 높은 온도에서 생성된 텍스트는 창의적인 텍스트들이 생성된다.\n",
    "    - 실제 단어가 아닌 실제 단어와 비슷하게 보이는 단어들을 생성해낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.5 정리\n",
    "\n",
    "- 이전의 토큰이 주어지면 다음 토큰(들)을 예측하는 모델을 훈련하여 시퀀스 데이터를 생성할 수 있다.\n",
    "\n",
    "- 텍스트의 경우 이런 모델을 **언어 모델**이라고 부른다. \n",
    "\n",
    "- 다음 토큰을 샘플링할 때 모델이 만든 출력에 집중하는 것과 무작위성을 주입하는 것 사이에 균형을 맞추어야 한다.\n",
    "\n",
    "- 이를 위해 **소프트맥스 온도** 개념을 사용한다. 항상 다양한 온도를 실험해서 적절한 값을 찾아야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가: 1D-ConvNet으로 테스트해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(32, 5, padding='same', \n",
    "                        activation='relu', input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.MaxPooling1D(2))\n",
    "model.add(layers.Conv1D(64, 5, padding='same', dilation_rate=3,\n",
    "                        activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "타깃이 원-핫 인코딩되어 있기 때문에 모델을 훈련하기 위해 categorical_crossentropy 손실을 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 : 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 6s 28us/step - loss: 2.0302\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the shild the conter the the man the and the which the sang the the the stinting of the the prong the the the the the the and the stic the strang and the conter the strut the ment the man the prones the stiring the the the conting of the some the strang the and the struting the the stre the the strute and the which the some the which the more and the struting the and the stres of the more the co\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for and sour rer who more the prong the an the lacting and sopher the strat and capirion one rous profit and scour at and prearing for at althan in not the perpress and ponters the belonces onohe the sponsed drancinting the the cont dratous the vilut of and the sther man be the continter sow moches in not and that the us the it the which a never whill himserrer the with words of the the with the rou\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through fore wintions, which, what the from somaror re bad lited, riffined peron what emplilistents both not domaroc they cont that mavition in to which, inter,\n",
      "ous elitive\n",
      "the us phoods riscited the nable rys withonces the vere) and\" ance, one ceven its onswardcipticeshe to mage hesapersic. the la"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cjh/miniconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sterted soming ha hoc olafar the symelog trucins once inaclifinelly darat loegcly at for which for\n",
      "listifuccino\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for nature that on wa (shinged ona tenceve mtal, and sfersconcedlivin suvinapuruern comtful what ving from on thimsole\n",
      "concaprelings the shild maut mrigle hafmerpiss a ploquent, int hupe laftth carcinctly ray; thow now be heae chrhes of his ranger,\n",
      "preseiif qreecede troun\n",
      "of that its enkert\n",
      "it mich\n",
      "amtg sucltyy: ord hissere whicchenfindiself\n",
      "manit\n",
      "wrom-. nekarcrall esas? the stifent brhcitile stort \n",
      "에포크 : 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 5s 26us/step - loss: 1.9339\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the server the sous of the serate and the sent of the and the not of and a more and of the stare the in the sent the sent a the sime sence and the it a sent is the seer a the sere and experion and a compossion of the preed and the serate and the somether the supere the the the more the in of the more and the as a come the somether the prese and the serate to the in the somether the somes and a t\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for of the\n",
      "forment with the something and expenself be one in a lase on the from in the and attion in of a sender and the somes of the the have faring it the hapirious and\n",
      "more be which a men is and and ally of the most the rence of sace to miss enfertion a come a will nevery with the sent and that as or the notion a sent a for\n",
      "of the not a the wad a the\n",
      "and conced the and the more of the soer bean \n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for \"ears, seeciest ald we the mentt of \"soe adanines: ut the spiver\n",
      "k foubty sifolly not\n",
      "onet pared entaufe consticattion it feeiosuater of in thought uld\n",
      "cretibly is inters. in taound, the which the ough deive and daginst\n",
      "mefore a the nesupation the deced its shersmfinide it\n",
      " that iwshus innelfnelutence and so to are\n",
      "this\n",
      "tivetes of the owors teemintendibol. he dever\n",
      "to ne maned one worlded mision\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through foout, oroughy\n",
      "to maun inder\" it ob, but, motuacticatintic and gais-tome no auls.\n",
      "turable rividuramonessy, it broun in the rerments hay perfevire, and dot it of\n",
      "hugrees stneal nots resermmmougred of tucwialingsion inperoraturornes him simise\n",
      "broce\n",
      "sagart of name-yyke, a laxidation bowk ats a\n",
      "from\n",
      "indize, withrition in\n",
      "thesions ever this. for\n",
      "\"pheition tiow\n",
      "dawer\n",
      "relilly. othy jeerdernty\n",
      "ta\n",
      "thas veev\n",
      "에포크 : 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 5s 26us/step - loss: 1.8891\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for and a suple is the propility and a strance and and constimenting of the sous the serve of the spirit as as on is the sous and songer the simself a more the dever the spirit the sous of the serve and of the prope and and the sense with and with the sous the somether the serate and the spirit and the serst the superious and some the serve and the philoses and a to consting the spirit the spirit th\n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for the will the serle it.\n",
      "\n",
      "11\n",
      "\n",
      "\n",
      "11\n",
      "                \n",
      "                        \n",
      " \n",
      "                                                       ne of\n",
      "the greive to\n",
      "the dard that is to the prove of the phas be the to the the into magity of pleced and men is not are sans who to has and to be which cive assilate the sent the procient as which sermanhe with the proves sore the wistion which or the condertions wh\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for\n",
      "that ments thad ording be god, once sences of cous of aking\n",
      "coult hethered\" meciwl \n",
      "moin for to swecittnes be sems obbume enans\n",
      "dove as trin beys to dee\" funcly, and deative osriving in the\n",
      "such ther of the spis, and dand capod hand the proat might doblem\n",
      "the barts\n",
      "now for this pilsitarad sa chames hespiy tooy. the srused\n",
      "his\n",
      "would decurstermes of moraking, hoce sidated uno, try wuth gabt just h\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for owratated mon\", long. hy posiols\n",
      "to gerie,\n",
      "deintestsing unwhen only; thing dimind ine with werefuthers, that far\n",
      "phasicase ksmon? to at luter, philatuctsyonce of emurer can man. fevere of a blate are raw seaponious,\n",
      "of eld ioteat suply, unatistion of chus sais, to\n",
      "modisifinstsind has, an a rornmy yyby nuthingtennses so the to for are maithy goits, hown in questate--euls has is is ving alsonsty, \n",
      "에포크 : 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 5s 25us/step - loss: 1.8596\n",
      "--- 시드 텍스트:\"the slowly ascending ranks and classes, in which,\n",
      "through fo\"\n",
      "------ 온도: 0.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for of the sees of the prese commant the more and the free the more in of the sous the to an the such the some the prese with the profgely to the seestran and men the with the conself the come to the some the come the beew to the serate the strate the sose of the some the server and seest and and the still the some the to the and and some to the to perse and the constand to and for the some and the \n",
      "------ 온도: 0.5\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for that\n",
      "the a mething and fely man will experson of not the beese but and as in the seems cal\n",
      "more to at the the rols the more the deces, sees of sense deaged the conderden of the comes of the income sour well the the stalived and cherstity, and is an among the himself thest, and him with the most his low to dece simetion\n",
      "and some men that all deate man ir and consuke, too a man amousure are the an\n",
      "------ 온도: 1.0\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for his our gicthed there\n",
      "alart it! constands, heerthoun edly worce. the disted which sempt modipact the digreated as he buffer at noughthat as regelegen tecies sowh ally philoged to looked beneirs usy, excapny.  never\n",
      "suranconce of sand us humby, ocame\n",
      "if of sessaal to a bee\n",
      "onaged numplemently what time niphate whom of the\n",
      "is ger the\n",
      "sphates curiscien of be basty--come meito\n",
      "treen: that, old there\n",
      "------ 온도: 1.2\n",
      "the slowly ascending ranks and classes, in which,\n",
      "through for\n",
      "pimate\n",
      "ar busbodusver- ra and quife deglied\"\"\n",
      "slosor, is dicquently pnee sle it\"\n",
      "warst, at in of\n",
      "to difmes ucres sornoumens mant.p\n",
      "radesthough,  the hopi \"life, piestnatelada, ony, and metaal hyde, of tood ci seestulut; mongness ment\n",
      "ness\n",
      "evert-,ementably!, rooves in ible\" of ek-u to pre not it on, is is nefulutres of se onced once,noy\n",
      "when proption: crue mas he\n",
      "at\n",
      "kraty, a sobilly.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "= \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "random.seed(42)\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "# 5 epoch 동안 모델을 훈련한다.\n",
    "for epoch in range(1, 5):\n",
    "    print('에포크 :', epoch)\n",
    "    # 데이터에서 한 번만 반복해서 모델을 학습한다.\n",
    "    model.fit(x, y, batch_size=128, epochs=1)\n",
    "    \n",
    "    # 무작위로 시드 텍스트를 선택한다\n",
    "    seed_text = text[start_index: start_index + maxlen]\n",
    "    print('--- 시드 텍스트:\"' + seed_text + '\"')\n",
    "    \n",
    "    # 여러가지 샘플링 온도를 시도한다.\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ 온도:', temperature)\n",
    "        generated_text = seed_text\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # 시드 텍스트에서 시작해서 400개의 글자를 생성한다.\n",
    "        for i in range(400):\n",
    "            # 지금까지 생성된 글자를 원-핫 인코딩으로 바꾼다.\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            # 다음 글자를 샘플링 한다.\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
